[0m[[0m[31merror[0m] [0m[0morg.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/home/jestra/lab3/project/369-project/federalist_papers[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:321)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:264)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.input.WholeTextFileInputFormat.setMinPartitions(WholeTextFileInputFormat.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.WholeTextFileRDD.getPartitions(WholeTextFileRDD.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:269)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:269)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:269)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:269)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2132)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.collect(RDD.scala:989)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)[0m
[0m[[0m[31merror[0m] [0m[0m	at project.TFIDFAuthorPredict$.main(TFIDFAuthorPredict.scala:31)[0m
[0m[[0m[31merror[0m] [0m[0m	at project.TFIDFAuthorPredict.main(TFIDFAuthorPredict.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Method.java:498)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:135)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:85)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:112)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:178)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:112)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:2072)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:2011)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:378)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Thread.java:750)[0m
[0m[[0m[31merror[0m] [0m[0m(Compile / [31mrun[0m) org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/home/jestra/lab3/project/369-project/federalist_papers[0m
